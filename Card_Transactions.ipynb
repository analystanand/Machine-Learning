{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:anaconda3]",
      "language": "python",
      "name": "conda-env-anaconda3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Card_Transactions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/analystanand/Machine-Learning/blob/master/Card_Transactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B10DVywVjER-",
        "colab_type": "text"
      },
      "source": [
        "# Capital One Data Science Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDQAwl_-QmXR",
        "colab_type": "text"
      },
      "source": [
        "# Content\n",
        "\n",
        "\n",
        "1. Introduction  \n",
        "2. Project Requirements\n",
        "2. Download and Extract data\n",
        "3. Library Installations\n",
        "4. Package Imports \n",
        "3. Load Transaction data into Dataframe\n",
        "4. Summary of data\n",
        "5. Histogram of monetary features\n",
        "6. Data Wrangling - Duplicate Transactions\n",
        "7. Predictive Modeling\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBB5UOCTl8Oy",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "Fraud is a problem for any bank. Fraud can take many forms, whether it is someone stealing a single credit card, to large batches of stolen credit card numbers being used on the web, or even a mass compromise of credit card numbers stolen from a merchant via tools like credit card skimming devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3pMDRDZVp9x",
        "colab_type": "text"
      },
      "source": [
        "#Project Requirement\n",
        "1. IDE: Google Colab/Jupyter Notebook\n",
        "2. Language: Python\n",
        "\n",
        "\n",
        "# How to run code code\n",
        "1. Open notebook in Jupyter Notebook or Google Collab(preferably)\n",
        "2. Install mentioned requirements from below by running library installations cell. or `!pip install -r requirements.txt`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJgmw-XivrBL",
        "colab_type": "text"
      },
      "source": [
        "#Library Installations and Package Imports\n",
        "Install all dependencies once and then comment for multiple runs to avoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPU1QWQ5jESY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "747fbb13-1103-413b-c617-718376b8099d"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category-encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.14.1)\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.17.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.25.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (0.22.1)\n",
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (1.12.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (0.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver->-r requirements.txt (line 6)) (45.1.0)\n",
            "Installing collected packages: category-encoders, jsonlines, scikit-plot\n",
            "Successfully installed category-encoders-2.1.0 jsonlines-1.2.0 scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db0F1LNMjESe",
        "colab_type": "code",
        "outputId": "1f71dc48-f0ac-4a48-ac6e-36b7939b8914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import jsonlines\n",
        "import matplotlib\n",
        "from sklearn.pipeline import Pipeline,make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import scikitplot as skplt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import category_encoders as ce\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLJla9NQjESH",
        "colab_type": "text"
      },
      "source": [
        "# Question 1:Download, Extract and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx6zLolgjESM",
        "colab_type": "code",
        "outputId": "35039cf6-024e-48ba-c87e-3001f69ba695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!git clone https://github.com/CapitalOneRecruiting/DS.git\n",
        "!unzip DS/transactions.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DS'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 3 (delta 0), pack-reused 1\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "Archive:  DS/transactions.zip\n",
            "  inflating: transactions.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixreWYOsv3Kk",
        "colab_type": "text"
      },
      "source": [
        "Read transactions.txt from file and load into pandas dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFYfFpR4jES7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILENAME= \"./transactions.txt\"\n",
        "transactions = []\n",
        "with jsonlines.open(FILENAME) as file:\n",
        "    transactions = [record for record in file]\n",
        "transactions_df = pd.DataFrame(transactions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZCAXjGNuR1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3IRAXlvjh21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of transaction records:\",transactions_df.shape[0])\n",
        "print(\"No. of Customers:\",transactions_df.customerId.unique().shape[0])\n",
        "print(\"No. of Columns/Features:\",transactions_df.shape[1]-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWcHjzm2nAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"pandas data types\")\n",
        "transactions_df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCUnfknrydH",
        "colab_type": "text"
      },
      "source": [
        "Data Types of features\n",
        "\n",
        "Data types of features are not explicit with pandas data types. So by looking at a few rows, we can infer data types. It's important to identify data types of features as various models treat categorical and continuous features differently. Different data types will go through different preprocessing steps for feeding them into the model. Some features which don't contain any data will be dropped.\n",
        " \n",
        "Categorical features need to be encoded and Numerical Features might be normalized for models to converge.\n",
        " \n",
        "Datetime might be used to derive a numerical type. For example. Difference between transactionDate and dateOfLastAddressChange might be useful features in the prediction of fraud.\n",
        "\n",
        "1. accountNumber\n",
        "2. customerId\n",
        "3. creditLimit\n",
        "4. availableMoney\n",
        "5. currentBalance\n",
        "6. cardCVV\n",
        "7. enteredCVV\n",
        "8. cardLast4Digits\n",
        "\n",
        "\n",
        "DateTime Data Type\n",
        "1. transactionDateTime\n",
        "2. currentExpDate\n",
        "3. accountOpenDate\n",
        "4. dateOfLastAddressChange\n",
        "\n",
        "Categorical Data Type\n",
        "1. merchantName\n",
        "2. acqCountry\n",
        "3. merchantCountryCode\n",
        "4. posEntryMode\n",
        "5. posConditionCode\n",
        "6. merchantCategoryCode\n",
        "7. transactionType\n",
        "8. cardPresent\n",
        "9. expirationDateKeyInMatch\n",
        "\n",
        "\n",
        "Null Features\n",
        "1. echoBuffer \n",
        "2. merchantCity \n",
        "3. merchantState \n",
        "4. merchantZip \n",
        "5. posOnPremises\n",
        "6. recurringAuthInd\n",
        "\n",
        "Target Variable\n",
        "1. isFraud\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2KayN2QUlFq",
        "colab_type": "text"
      },
      "source": [
        "There are few null columns/features(might be removed for privacy reasons) which are not useful for analysis and need to be dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtw3LJxUSk7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty_columns = []\n",
        "for i in transactions_df.columns.values:\n",
        "  #check only single item in a column\n",
        "  if len(set(transactions_df[i]))==1:\n",
        "     empty_columns.append(i)\n",
        "transactions_df = transactions_df.drop(empty_columns,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPdyESmB5dNZ",
        "colab_type": "text"
      },
      "source": [
        "Let's check if any data is missing, we might need to fill them with appropriate techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgTG29aF49Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of missing data(if any)\",transactions_df.isnull().values.sum()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUcvjrjR5_nn",
        "colab_type": "text"
      },
      "source": [
        "Common Statistics  about numerical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLV8vQg6xlQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l60IjCGTd900",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df['transaction_id'] = transactions_df.index #for uniquely identifying transactions for using into groupBy operation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9RdyFneyu47",
        "colab_type": "text"
      },
      "source": [
        "#Question 2: Plot\n",
        "Histogram of monetary features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k4hvwdJWQ6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reference:https://seaborn.pydata.org/examples/distplot_options.html\n",
        "# Set up the matplotlib figure\n",
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.distplot(transactions_df['creditLimit'],kde=False, color=\"g\", ax=axes[0])\n",
        "sns.distplot(transactions_df['availableMoney'],kde=True,color=\"m\", ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYXuee7NywbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reference:https://seaborn.pydata.org/examples/distplot_options.html\n",
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.distplot(transactions_df['transactionAmount'],kde=True,norm_hist=True, color=\"r\", ax=axes[0])\n",
        "sns.distplot(transactions_df['currentBalance'],kde=True,norm_hist=True,color=\"b\", ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNVyjA_8aDs",
        "colab_type": "text"
      },
      "source": [
        "Approximately, Transactions Amount is in shape of positive skewed normal distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md6nrhzbzltQ",
        "colab_type": "text"
      },
      "source": [
        "#Question 3: Data Wrangling - Duplicate Transactions\n",
        "\n",
        "Combination of customerId, Transaction Amount and Merchant Name will help in identifying transactions uniquely and counting its repetition will help us in identifying reversed and multi-swipe. Multi-swipe  can be identified as transaction repeating more than once after filtering out regular transactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQXp3Cfr6egp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transaction_with_dup_flag = transactions_df[[\"customerId\",\"transactionAmount\",\"merchantName\"]].duplicated()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpoODiXZKvgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df[\"isDuplicate\"]=transaction_with_dup_flag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G_BpUt9OaZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duplicate_transactions = transactions_df[transactions_df[\"isDuplicate\"]==True]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ni1VI-OcjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result =  duplicate_transactions[[\"customerId\",\"transactionAmount\",\"merchantName\",\"transactionDateTime\"]].groupby(by=[\"customerId\",\"transactionAmount\",\"merchantName\"]).count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYBKKN7RPhES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup_analysis_df = result.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja-ech90VtZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup_analysis_df = dup_analysis_df.rename(columns={\"transactionDateTime\": \"occurence\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO_G1gF-V9S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup_analysis_df['dup_category'] = np.where(dup_analysis_df['occurence']>1, 'multi-swipe', 'reversed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-7KgbzLXLl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "analysis = dup_analysis_df.groupby(by=[\"dup_category\"]).sum()\n",
        "analysis[\"percentageCount\"] = round((analysis[\"occurence\"]/analysis[\"occurence\"].sum())*100,2)\n",
        "analysis[\"percentageAmount\"] = round((analysis[\"transactionAmount\"]/analysis[\"transactionAmount\"].sum())*100,2)\n",
        "analysis.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg55kXmuAYcf",
        "colab_type": "text"
      },
      "source": [
        "Although percentage of multi-swipe occurence is around 63% but account for 93% of total duplicate transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWZ4SYHhLapC",
        "colab_type": "text"
      },
      "source": [
        "# Question 4: Model\n",
        "\n",
        "Predicting a transaction is a fraud comes under classification problem in Machine Learning.\n",
        "\n",
        "Evaluation Metric\n",
        "As it is imbalanced dataset and fraudulent cases are rare, We will carefully choose our evaluation metric.\n",
        "\n",
        "1. We want to increase recall for fraudulent transactions which might also mean lower precision but it is trade off to minimize loss due to frauds.\n",
        "2. We will use ROC curve, precision-recall(preferably) and confusion matrix for peformance on training set. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkralsWuo8sG",
        "colab_type": "text"
      },
      "source": [
        "Steps\n",
        "\n",
        "1. Derive features which can not be directly feed into models. \n",
        "\n",
        "2. Distribution of Fraud and Normal Transaction\n",
        "\n",
        "3. Drop features not useful for prediction for baseline model.It might be incoporated if performance is not good enough. \n",
        "\n",
        "4. Split data into Training and Testing set while mainiting ratio of class. (startify=True).Testing set will be used only for final performance so as to avoid any data leakage.\n",
        "\n",
        "5. Descriptive analysis of features among classes in Training Set for visualization of identifying important features.\n",
        "\n",
        "6. Pipeline for handling preprocessing: Label Encoding and Feature Scaling\n",
        "\n",
        "7. PCA for visualization in lower dimension.\n",
        "\n",
        "8. Train model for handling Imbalanced data\n",
        "\n",
        "9. Handling Imbalanced data via Class weight loss function\n",
        "\n",
        "10. Handling Imbalanced data via Random Undersampling.\n",
        "\n",
        "11. Handling Imbalanced data via Random Oversampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFWDR0cp-kgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df[\"transactionDateTime\"] = pd.to_datetime(transactions_df[\"transactionDateTime\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYvLSgN4kW9",
        "colab_type": "text"
      },
      "source": [
        "#### Derive Features \n",
        "We can derive features from datetime  features which might be useful for training a model.\n",
        "\n",
        "1. Difference in days between transaction date and  date of last address.\n",
        "2. Difference in days between transaction date and  date of account open date.\n",
        "3. Difference in days between transaction date and  date of current exp date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlaALMCcqwt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df[\"dateOfLastAddressChange\"] = pd.to_datetime(transactions_df[\"dateOfLastAddressChange\"])\n",
        "transactions_df['diff_address_transactions'] = transactions_df['transactionDateTime'] - transactions_df['dateOfLastAddressChange']\n",
        "transactions_df['diff_address_transactions'] = transactions_df['diff_address_transactions']/np.timedelta64(1,'D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb4O0_Tth-Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df[\"accountOpenDate\"] = pd.to_datetime(transactions_df[\"accountOpenDate\"])\n",
        "transactions_df['diff_accountopen_transactions'] = transactions_df['transactionDateTime'] - transactions_df['accountOpenDate']\n",
        "transactions_df['diff_accountopen_transactions'] = transactions_df['diff_accountopen_transactions']/np.timedelta64(1,'D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk8TamYXIM7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df[\"currentExpDate\"] = pd.to_datetime(transactions_df[\"currentExpDate\"],infer_datetime_format=True)\n",
        "transactions_df['diff_currentExpDate_transactions'] =  transactions_df['currentExpDate'] - transactions_df['transactionDateTime'] \n",
        "transactions_df['diff_currentExpDate_transactions'] = transactions_df['diff_currentExpDate_transactions']/np.timedelta64(1,'D')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erJbeX0rlw1s",
        "colab_type": "text"
      },
      "source": [
        "####Distribution of Fraud and Normal Transaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NJKKk1lLcJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a4_dims = (15, 6)\n",
        "fig, ax = plt.subplots(figsize=a4_dims)\n",
        "class_dist=pd.value_counts(transactions_df[\"isFraud\"], sort= True)\n",
        "class_dist.plot(kind= 'bar')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZDrppf2E42T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_percentage(t_df):\n",
        "  No_of_Normal_transacation = len(t_df[t_df[\"isFraud\"]==True]) \n",
        "  No_of_Fraud_transacation =len(t_df[t_df[\"isFraud\"]==False])\n",
        "  percent_of_fraud_transaction =  No_of_Fraud_transacation/(No_of_Normal_transacation+No_of_Fraud_transacation)\n",
        "  percent_of_normal_transaction = No_of_Normal_transacation/(No_of_Normal_transacation+No_of_Fraud_transacation)\n",
        "  print(\"percent of Normal transactions: {0:.2%}\".format(percent_of_fraud_transaction))\n",
        "  print(\"percent of Fraud transactions: {0:.2%}\".format(percent_of_normal_transaction))\n",
        "\n",
        "class_percentage(transactions_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zek4GhkLswUt",
        "colab_type": "text"
      },
      "source": [
        "As we can see that fraud transactions are just 1.58% of total transactions. This is highly imbalanced dataset so we have to carefully choose metric for performance evaluation as well as train a model which is not baised towards normal transaction.For example: randomly predicting all transaction as normal will provide high accuracy to the model but it's not useful for us. \n",
        "\n",
        "\n",
        "---\n",
        "There are few techniques to handle imbalanced data.\n",
        "1. Custom loss function to penalize model when it observe minority class\n",
        "2. Random Undersampling the majority class\n",
        "3. Random and SMOTE based Oversampling the minority class\n",
        "4. Ensemble of classifier trained on minority class and different sets of majority class and use majority vote.\n",
        "\n",
        "We will try only undersampling method although we will lose information but to train a model without an inherent bias towards majority class.\n",
        "\n",
        "First of all lets seperate the data into training and test set before undersampling this is real scenarios we will be facing when it comes to deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMMXisBfbsHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyeD8v7UZoXJ",
        "colab_type": "text"
      },
      "source": [
        "####Drop features not useful for prediction\n",
        "Dropping features need not to be fed into baseline model or we have already derived features from those: \n",
        "\n",
        "1. accountNumber,\n",
        "2. customerId,\n",
        "3. transactionDateTime,\n",
        "4. currentExpDate,\n",
        "5. accountOpenDate\n",
        "6. dateOfLastAddressChange\n",
        "7. cardCVV\t\n",
        "8. enteredCVV\t\n",
        "9. cardLast4Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6kTI3UrcEyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transactions_df = transactions_df.drop([\"accountNumber\",\"customerId\",\"transactionDateTime\",\"currentExpDate\",\"accountOpenDate\",\"dateOfLastAddressChange\",\"cardCVV\",\"enteredCVV\",\"cardLast4Digits\",],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXL59FI_J2FI",
        "colab_type": "text"
      },
      "source": [
        "####Split data into Training and Testing set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-LWHBkjyy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = transactions_df.drop('isFraud', axis=1) #for train-test split\n",
        "y_label = transactions_df['isFraud']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3WG4nc2mX1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "label_encoder = le.fit(y_label)\n",
        "y_label = label_encoder.transform(y_label)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2,stratify=y_label)\n",
        "\n",
        "print(\"Training Size:\",X_train.shape,\"Testing Size:\",X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCUF96ttkKeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = X_train.copy() #temporary copy for analysis of training set\n",
        "train_df[\"isFraud\"] = pd.Series(le.inverse_transform(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFboa77bKKQQ",
        "colab_type": "text"
      },
      "source": [
        "### Descriptive analysis of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf_3zZugYyc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(20, 6), sharex=False)\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"creditLimit\"],hist=True,kde=False,color=\"r\",label='Fraud',ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['creditLimit'],hist=True,kde=False,color=\"b\",label='Normal',ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['availableMoney'],hist=False,kde=True,color=\"b\",label='Normal',kde_kws={\"color\": \"b\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"availableMoney\"],hist=False,kde=True,color=\"r\",label='Fraud',kde_kws={\"color\": \"r\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r30yIP8bZmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(20, 6), sharex=False)\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"transactionAmount\"],hist=False,kde=True,color=\"r\",label='Fraud',kde_kws={\"color\": \"r\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['transactionAmount'],hist=False,kde=True,color=\"b\",label='Normal',kde_kws={\"color\": \"b\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"currentBalance\"],hist=False,kde=True,color=\"r\",label='Fraud',kde_kws={\"color\": \"r\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['currentBalance'],hist=False,kde=True,color=\"b\",label='Normal',kde_kws={\"color\": \"b\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq-VpTg6NMzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.countplot('merchantCountryCode', data = train_df,hue=\"isFraud\",ax=axes[0])\n",
        "sns.countplot('acqCountry', data = train_df,hue=\"isFraud\",ax=axes[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y0RXD9nO5rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.countplot('posEntryMode', data = train_df,hue=\"isFraud\",ax=axes[0])\n",
        "sns.countplot('posConditionCode', data = train_df,hue=\"isFraud\",ax=axes[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG07NusmN7L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grp_by_merc_code = train_df.groupby(by=[\"merchantCategoryCode\",\"isFraud\"])[\"transaction_id\"].count().reset_index()\n",
        "grp_by_merc_code['Fraud percentage'] = grp_by_merc_code[\"transaction_id\"]*100 / grp_by_merc_code.groupby(\"merchantCategoryCode\")[\"transaction_id\"].transform(\"sum\")\n",
        "a4_dims = (15, 6)\n",
        "fig, ax = plt.subplots(figsize=a4_dims)\n",
        "plt.xticks(rotation=45)\n",
        "sns.barplot('merchantCategoryCode', 'Fraud percentage',hue = 'isFraud', data = grp_by_merc_code)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMMIgO_9Z8sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.countplot('transactionType', data = train_df,hue=\"isFraud\",ax=axes[0])\n",
        "sns.countplot('cardPresent', data = train_df,hue=\"isFraud\",ax=axes[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8dfrjWLhTYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(15, 6), sharex=False)\n",
        "sns.countplot('expirationDateKeyInMatch', data = train_df,hue=\"isFraud\",ax=axes[0])\n",
        "sns.countplot('isDuplicate', data = train_df,hue=\"isFraud\",ax=axes[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAp2Htemf-ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,3, figsize=(20, 6), sharex=True)\n",
        "ax = sns.boxplot(x=\"isFraud\", y=\"availableMoney\", data=train_df,ax =axes[0])\n",
        "ax = sns.boxplot(x=\"isFraud\", y=\"transactionAmount\", data=train_df,ax =axes[2])\n",
        "ax = sns.boxplot(x=\"isFraud\", y=\"currentBalance\", data=train_df,ax =axes[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BdNn5L8irem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, axes = plt.subplots(1,2, figsize=(20, 6), sharex=True)\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"diff_address_transactions\"],hist=False,kde=True,color=\"r\",label='Fraud',kde_kws={\"color\": \"r\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['diff_address_transactions'],hist=False,kde=True,color=\"b\",label='Normal',kde_kws={\"color\": \"b\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[0])\n",
        "sns.distplot(train_df[train_df['isFraud']==True][\"diff_accountopen_transactions\"],hist=False,kde=True,color=\"r\",label='Fraud',kde_kws={\"color\": \"r\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "sns.distplot(train_df[train_df['isFraud']==False]['diff_accountopen_transactions'],hist=False,kde=True,color=\"b\",label='Normal',kde_kws={\"color\": \"b\", \"lw\": 3, \"label\": \"KDE\"},ax=axes[1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4ufZkNuMKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_new = X_train.drop([\"transaction_id\"],axis=1) #drop columun transaction used for groupby on merchantCategoryCode\n",
        "X_test_new = X_test.drop([\"transaction_id\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IHU2wHBz8dn",
        "colab_type": "text"
      },
      "source": [
        "###Pipeline for handling preprocessing\n",
        "Label Encoding and Scaling of categorical and continuous features respectively.\n",
        "different encoding might be utilized for improving performance of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE24evFpMyqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerical_features = X_train_new.select_dtypes(include=['float64']).columns\n",
        "categorical_features = X_train_new.select_dtypes(include=['object','bool']).columns\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('woe', ce.OrdinalEncoder())])\n",
        "preprocessor = ColumnTransformer(transformers=[('cat', categorical_transformer, categorical_features),('num', numeric_transformer, numerical_features)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO2aiNtrS-UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor.fit(X_train_new)\n",
        "X_trans =  preprocessor.transform(X_train_new)\n",
        "X_test_trans = preprocessor.transform(X_test_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTNKoZB3FVgv",
        "colab_type": "text"
      },
      "source": [
        "###PCA Visualization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNc30ue8716d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(X_trans)\n",
        "finalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "finalDf[\"target\"]=y_train\n",
        "fig = plt.figure(figsize = (15,6))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [0,1]\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['target'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04A5gB370qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ1QcijSAtKu",
        "colab_type": "text"
      },
      "source": [
        "Observation of PCA of provides insights that linear classifier such as logistic regression(baseline) might not perform well and also choice of encoder also put inherent order for multiple categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJtkmjSdQwHW",
        "colab_type": "text"
      },
      "source": [
        "###Approach 1:Handling Imbalanced Data via Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nydZQMgqTKEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " clf = LogisticRegression(max_iter=800 ,class_weight=\"balanced\")\n",
        " best_clf = clf.fit(X_trans, y_train)\n",
        " y_score = clf.predict_proba(X_test_trans)\n",
        " y_pred = clf.predict(X_test_trans)\n",
        " print(classification_report(y_test, y_pred))\n",
        " f, axes = plt.subplots(1,3, figsize=(20, 6), sharex=False)\n",
        " skplt.metrics.plot_roc(y_test, y_score,ax=axes[0])\n",
        " skplt.metrics.plot_precision_recall(y_test, y_score,ax=axes[1])\n",
        " skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,ax=axes[2])\n",
        " plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSNRh0OHRWY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " param_grid = {'class_weight':[{1: w} for w in [3,6,9]]}\n",
        " clf = GridSearchCV( DecisionTreeClassifier(criterion=\"entropy\"), scoring=\"recall_micro\",param_grid = param_grid, cv = 2, verbose=True, n_jobs=-1)\n",
        " best_clf = clf.fit(X_trans, y_train)\n",
        " print(\"best parameters\",clf.best_params_)\n",
        " y_score = clf.predict_proba(X_test_trans)\n",
        " y_pred = clf.predict(X_test_trans)\n",
        " print(classification_report(y_test, y_pred))\n",
        " f, axes = plt.subplots(1,3, figsize=(20, 6), sharex=False)\n",
        " skplt.metrics.plot_roc(y_test, y_score,ax=axes[0])\n",
        " skplt.metrics.plot_precision_recall(y_test, y_score,ax=axes[1])\n",
        " skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,ax=axes[2])\n",
        " plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQOYtGEV4H9o",
        "colab_type": "text"
      },
      "source": [
        "###Approach 2:Handling imbalanced data via undersampling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9_r8lHY4M8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus = RandomUnderSampler(random_state=5)\n",
        "rus.fit(X_trans, y_train)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_trans, y_train)\n",
        "print(\"Size of resampled data\",X_resampled.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdnkjzu2mLOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " clf = RandomForestClassifier(n_estimators=500, class_weight={1:6})\n",
        " best_clf = clf.fit(X_resampled, y_resampled)\n",
        " y_score = clf.predict_proba(X_test_trans)\n",
        " y_pred = clf.predict(X_test_trans)\n",
        " print(classification_report(y_test, y_pred))\n",
        " f, axes = plt.subplots(1,3, figsize=(20, 6), sharex=False)\n",
        " skplt.metrics.plot_roc(y_test, y_score,ax=axes[0])\n",
        " skplt.metrics.plot_precision_recall(y_test, y_score,ax=axes[1])\n",
        " skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,ax=axes[2])\n",
        " plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYPsSQ-GL90d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_names = list(categorical_features.values)+list(numerical_features.values) #for feature importance\n",
        "importances = clf.feature_importances_\n",
        "std = np.std([clf.feature_importances_ for tree in clf.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1][:8]\n",
        "re_arranged_features = [ col_names[i] for i in indices][:8]\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(8), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(8),labels=re_arranged_features,rotation=70)\n",
        "plt.xlim([-1, 8])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6v9cUKG60W",
        "colab_type": "text"
      },
      "source": [
        "###Approach 3:Handling imbalanced data via random oversampling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8DXcuTTH4FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ros = RandomOverSampler(random_state=5)\n",
        "ros.fit(X_trans, y_train)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_trans, y_train)\n",
        "print(\"Size of resampled data\",X_resampled.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bao_45H8IEn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " clf = RandomForestClassifier()\n",
        " best_clf = clf.fit(X_resampled, y_resampled)\n",
        " y_score = clf.predict_proba(X_test_trans)\n",
        " y_pred = clf.predict(X_test_trans)\n",
        " print(classification_report(y_test, y_pred))\n",
        " f, axes = plt.subplots(1,3, figsize=(20, 6), sharex=False)\n",
        " skplt.metrics.plot_roc(y_test, y_score,ax=axes[0])\n",
        " skplt.metrics.plot_precision_recall(y_test, y_score,ax=axes[1])\n",
        " skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True,ax=axes[2])\n",
        " plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2tx8-pCLaAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_names = list(categorical_features.values)+list(numerical_features.values) #for feature importance\n",
        "importances = clf.feature_importances_\n",
        "std = np.std([clf.feature_importances_ for tree in clf.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1][:8]\n",
        "re_arranged_features = [ col_names[i] for i in indices][:8]\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(8), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(8),labels=re_arranged_features,rotation=70)\n",
        "plt.xlim([-1, 8])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjm841VivAi9",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "Its quite challenging to handle imbalanced dataset.Let's Look at the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvIB5xzK0dvT",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "1.  [An Easier Way to Encode Categorical Features](https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900)\n",
        "2. [Seaborn documentation](https://seaborn.pydata.org/tutorial.html)\n",
        "3. [PCA visualization](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n",
        "4. [Random Forest Feature Importance](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)"
      ]
    }
  ]
}